import numpy as np
import torch
import torch.nn.functional as F
import torchvision.transforms as T
from decord import VideoReader, cpu
from PIL import Image
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, CLIPTokenizer

# =========================
# 1) load InternVideo2 CLIP-S
# =========================
model_path = "OpenGVLab/InternVideo2_CLIP_S"
model = AutoModel.from_pretrained(model_path, trust_remote_code=True).cuda().eval()

# IMPORTANT: use CLIP tokenizer manually
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)

def build_transform(input_size):
    return T.Compose([
        T.Lambda(lambda img: img.convert("RGB") if img.mode != "RGB" else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])

def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):
    if bound:
        start, end = bound
    else:
        start, end = -1e9, 1e9
    start_idx = max(first_idx, round(start * fps))
    end_idx   = min(round(end * fps), max_frame)
    seg_size  = float(end_idx - start_idx) / num_segments
    frame_indices = np.array([
        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))
        for idx in range(num_segments)
    ])
    return frame_indices

def load_video(video_path, bound=None, input_size=224, num_segments=32):
    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)
    max_frame = len(vr) - 1
    fps = float(vr.get_avg_fps())

    transform = build_transform(input_size=input_size)
    frame_indices = get_index(bound, fps, max_frame, num_segments=num_segments)

    frames = []
    for i in frame_indices:
        img = Image.fromarray(vr[i].asnumpy()).convert("RGB")
        frames.append(transform(img))
    pixel_values = torch.stack(frames, dim=0)  # (T,3,H,W)
    return pixel_values

# =========================
# 2) encode video
# =========================
@torch.no_grad()
def encode_video(video_path, num_segments=32):
    pixel_values = load_video(video_path, input_size=224, num_segments=num_segments)
    pixel_values = pixel_values.to(model.device)

    if hasattr(model, "get_video_features"):
        v = model.get_video_features(pixel_values)
    elif hasattr(model, "encode_video"):
        v = model.encode_video(pixel_values)
    else:
        # fallback: try forward output
        out = model(pixel_values, return_dict=True)
        if hasattr(out, "video_embeds"):
            v = out.video_embeds
        elif hasattr(out, "last_hidden_state"):
            v = out.last_hidden_state.mean(dim=1)
        else:
            raise RuntimeError("Can't find video embedding output.")

    if v.dim() == 3:
        v = v.mean(dim=1)
    if v.dim() == 1:
        v = v.unsqueeze(0)
    return v

# =========================
# 3) encode text (use INTERNAL text encoder)
# =========================
@torch.no_grad()
def encode_text(task_desc):
    inputs = tokenizer(
        task_desc, return_tensors="pt",
        padding=True, truncation=True
    )
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    if hasattr(model, "get_text_features"):
        t = model.get_text_features(**inputs)
    elif hasattr(model, "encode_text"):
        t = model.encode_text(**inputs)
    elif hasattr(model, "text_encoder"):
        out = model.text_encoder(**inputs, return_dict=True)
        if hasattr(out, "pooler_output") and out.pooler_output is not None:
            t = out.pooler_output
        else:
            t = out.last_hidden_state.mean(dim=1)
    else:
        raise RuntimeError("Can't find internal text encoder API.")

    if t.dim() == 3:
        t = t.mean(dim=1)
    if t.dim() == 1:
        t = t.unsqueeze(0)
    return t

# =========================
# 4) cosine similarity reward
# =========================
@torch.no_grad()
def cosine_reward(video_path, task_desc, num_segments=32):
    v = encode_video(video_path, num_segments=num_segments)
    t = encode_text(task_desc)

    v = F.normalize(v, dim=-1)
    t = F.normalize(t, dim=-1)
    sim = (v * t).sum(dim=-1)
    return sim.item(), v, t

if __name__ == "__main__":
    video_path = "test_video/cup.mp4"
    task_desc  = "a robot arm picks up a cup"

    sim, v_emb, t_emb = cosine_reward(video_path, task_desc, num_segments=64)
    print("cosine similarity (reward):", sim)
    print("video emb shape:", v_emb.shape)
    print("text emb shape:", t_emb.shape)
